{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will change from the previous in that we will not have all possible action decimation rates available but lets say something along the lines of multiples. i.e. 1,2,4,8.\n",
    "\n",
    "We will also include the action in the state tape. So that it knows how fast it is going.\n",
    "\n",
    "Will probably also add replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    }
   ],
   "source": [
    "# Add path forimporting\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rnd\n",
    "from time import time\n",
    "import torch.nn.functional as F\n",
    "from sp_sims.simulators.stochasticprocesses import BDStates\n",
    "from sp_sims.utils.utils import get_q_mat\n",
    "from samprecon.environments.Environments import MarkovianUniformCumulativeEnvironment\n",
    "from samprecon.samplers.agents import SoftmaxAgent \n",
    "from samprecon.reconstructors.NNReconstructors import RNNReconstructor\n",
    "from samprecon.utils.rl_utils import calculate_returns\n",
    "from samprecon.estimators.value_estimators import SequenceValue\n",
    "from samprecon.feedbacksigs.feedbacks import LogEstimator\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "# Send random seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "# Initialize context first\n",
    "device = \"cpu\"\n",
    "# Check for Mac M1 and Cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"Running on GPU\")\n",
    "elif \"arm64\" in sys.version.lower():\n",
    "    device = \"mps\"\n",
    "    print(\"Running on Mac M1\")\n",
    "\n",
    "\n",
    "plt.style.use('rose-pine-dawn')\n",
    "rnd.seed(int(time()))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimation factor is tensor([21])\n"
     ]
    }
   ],
   "source": [
    "# Generate Environments on which to learn\n",
    "high_res_delta = 1e-0  # For generating the dataset and later sample\n",
    "num_states = 4\n",
    "baseline_rates = {\"lam\": 1 / 10, \"mu\": 4 / 10}\n",
    "true_q = torch.from_numpy(get_q_mat(baseline_rates,num_states))\n",
    "epochs = 2000\n",
    "length_of_episode = 15\n",
    "step_path_length = 1\n",
    "sampling_budget = 32\n",
    "epochs_b4_policy_update = 3\n",
    "learning_rate = 1e-3\n",
    "#used_path_length = 64  # So that we can let the process reach stationarity and take samples from stationary distribution\n",
    "avg_span = np.mean(1 / np.array(list(baseline_rates.values())))\n",
    "max_decimation = (\n",
    "    avg_span / high_res_delta\n",
    ") * 4  # Max decimation factor #CHECK: Maybe not divide by 2\n",
    "decimation_steps = [-8,-4,-2,-1,0,1,2,4,8]\n",
    "# Easier\n",
    "#current_decimation_factor = torch.LongTensor(\n",
    "#    [int(avg_span // high_res_delta)]\n",
    "#).to(device)\n",
    "# Harder\n",
    "current_decimation_factor = torch.randint(1, int(max_decimation), (1,))\n",
    "\n",
    "print(f\"Decimation factor is {current_decimation_factor}\")\n",
    "# Set random seed with time for randomnessj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_generator = BDStates(baseline_rates, high_res_delta, num_states)\n",
    "# sampling_arbiter.initialize_grad_hooks()\n",
    "q_recon_criterion = torch.nn.MSELoss()\n",
    "feedback_Qest = LogEstimator(true_q,10,num_states,q_recon_criterion)\n",
    "#reconstructor.initialize_grad_hooks()\n",
    "valueEst = SequenceValue(num_states+1,1).to(device)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8821e679c24259a54fd0238fdd679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ca816aa4fd467bb47e5911ab287d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "env = MarkovianUniformCumulativeEnvironment(\n",
    "    state_generator=state_generator,\n",
    "    feedback = feedback_Qest,\n",
    "    starting_decrate=current_decimation_factor,\n",
    "    sampling_budget=sampling_budget,\n",
    ")\n",
    "ebar = tqdm(range(epochs), desc=\"Epochs\", position=0)\n",
    "sampling_agent = SoftmaxAgent(sampling_budget+1, len(decimation_steps)).to(device)# +1 for the decimation factor\n",
    "# sampling_agent.initialize_grad_hooks()\n",
    "optimizer_policy = torch.optim.Adam(\n",
    "    sampling_agent.parameters(),\n",
    "    lr=learning_rate*1e-2\n",
    ")\n",
    "optimizer_value_est = torch.optim.Adam(valueEst.parameters(), lr=learning_rate)\n",
    "# val_opt = torch.optim.Adam(valueEst.parameters(), lr=1e-2)\n",
    "\n",
    "# Scheduler with warmpu\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "e_returns = []\n",
    "est_loss = []\n",
    "val_losses = []\n",
    "sampler_last_weights = list(sampling_agent.state_dict().values()) # ðŸž\n",
    "sbar = tqdm(range(length_of_episode), desc=\"Steps\", leave=True, position=1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # We generate a single step from the generator process\n",
    "    regrets = []\n",
    "    log_probs = []\n",
    "    val_ests = []\n",
    "    sbar.reset()\n",
    "\n",
    "    current_decimation_factor = torch.randint(1, int(max_decimation), (1,))\n",
    "\n",
    "    # Initial State\n",
    "    states = [\n",
    "        torch.cat(\n",
    "            (\n",
    "                torch.Tensor([current_decimation_factor]).to(device).view(1,1),\n",
    "                env.reset(current_decimation_factor).view(1, -1).to(device),\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    ]\n",
    "    # TODO: randomly select current_decimation_factor within some bounds\n",
    "    for step in range(length_of_episode):\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        cur_state = states[-1]\n",
    "        action_probs = sampling_agent.act(cur_state[:sampling_budget+2])\n",
    "        #print(\"Action probs are\", action_probs)\n",
    "        action_probs = action_probs.to(device)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        sampled_action = (dist.sample()).to(\n",
    "            device\n",
    "        )  # So as to not sample 0 (and end up dividing by zero)\n",
    "        \n",
    "        # CHECK: If we should have computation graph *not* cutting here\n",
    "        current_decimation_factor += decimation_steps[sampled_action.item()]\n",
    "        current_decimation_factor = torch.LongTensor([int(max(1,min(current_decimation_factor, max_decimation)))])\n",
    "        \n",
    "        # TODO: tensorize\n",
    "        dec_steps = (\n",
    "            torch.arange(\n",
    "                0,\n",
    "                current_decimation_factor.item() * sampling_budget,\n",
    "                current_decimation_factor.item(),\n",
    "            )\n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        one_hot_cur_state = F.one_hot(\n",
    "            cur_state[0,1:sampling_budget+1].to(torch.long), num_classes=num_states\n",
    "        ).to(device)\n",
    "        non_amb_state = (\n",
    "            torch.cat((one_hot_cur_state, dec_steps), dim=-1).to(torch.float).to(device)\n",
    "        ).view(1,sampling_budget,-1)\n",
    "        val_ests.append(valueEst(non_amb_state).to(device))\n",
    "\n",
    "        new_state, regret, done = env.step(current_decimation_factor)\n",
    "\n",
    "        new_state_w_rate = torch.cat(\n",
    "                (\n",
    "                    torch.Tensor([current_decimation_factor]).to(device).view(1,1),\n",
    "                    new_state\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        states.append(new_state_w_rate.to(device))\n",
    "        regrets.append(regret)\n",
    "        log_probs.append(dist.log_prob(sampled_action))\n",
    "\n",
    "        sbar.set_description(f\"At step {step}, Regret: {regret}\")\n",
    "        sbar.update(1)\n",
    "\n",
    "    returns = calculate_returns(regrets, gamma)\n",
    "    e_returns.append(returns[0].item())\n",
    "    policy_regrets = []\n",
    "    value_loss = []\n",
    "\n",
    "    for lp, val_est, r in zip(log_probs[:3], val_ests[:3], returns[:3]):\n",
    "        disadvantage = r - val_est.item()\n",
    "        policy_regrets.append(\n",
    "            -lp * disadvantage\n",
    "        )  # TODO: this might require a negative sign\n",
    "        value_loss.append(\n",
    "            F.mse_loss(val_est, torch.Tensor([r.item()]).view(1, -1).to(device))\n",
    "        )\n",
    "    # We update the whole thingko\n",
    "    policy_loss = torch.stack(policy_regrets).sum()\n",
    "    value_loss = torch.stack(value_loss).mean()\n",
    "\n",
    "    val_losses.append(value_loss.item())\n",
    "\n",
    "    # optimze:\n",
    "    optimizer_policy.zero_grad()\n",
    "    optimizer_value_est.zero_grad()\n",
    "    if epoch % epochs_b4_policy_update == 0:\n",
    "        optimizer_policy.step()\n",
    "        policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    optimizer_value_est.step()\n",
    "    #scheduler.step()\n",
    "\n",
    "    # ðŸ› Debugging\n",
    "    differences = []\n",
    "    for i, v in enumerate(sampling_agent.state_dict().values()):\n",
    "        differences.append(torch.sum(torch.abs(v - sampler_last_weights[i])))\n",
    "    differences_arbitrer = torch.sum(torch.tensor(differences))\n",
    "    # hard copy last weights\n",
    "    sampler_last_weights = [\n",
    "        copy.deepcopy(v) for v in sampling_agent.state_dict().values()\n",
    "    ]\n",
    "    differences = []\n",
    "\n",
    "\n",
    "    moving_avg_loss = np.mean(e_returns[-3:]) if epoch > 3 else np.mean(e_returns)\n",
    "    ebar.set_description(f\"Epoch Mean Regret: {moving_avg_loss}\")\n",
    "    ebar.update(1)\n",
    "    # We get reward based on how close we got to maximum information\n",
    "# Show Losses\n",
    "fig,axs = plt.subplots(1,2, figsize=(20,5))\n",
    "axs[0].set_title(\"Average (actual) Reconstruction Regret\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss (NLL)\")\n",
    "axs[0].plot(e_returns)\n",
    "\n",
    "axs[1].set_title(\"Value Estimation Loss\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Loss (MSE)\")\n",
    "axs[1].plot(val_losses)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/rac716/Research/SamplingReconstruction/experiments/reconstructions/exp3.6.left_right.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/experiments/reconstructions/exp3.6.left_right.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m date_time  \u001b[39m=\u001b[39m now\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/experiments/reconstructions/exp3.6.left_right.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# SaveModels \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/experiments/reconstructions/exp3.6.left_right.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(reconstructor\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels/reconstructor_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdate_time\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/experiments/reconstructions/exp3.6.left_right.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m torch\u001b[39m.\u001b[39msave(sampling_agent\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/sampling_agent_\u001b[39m\u001b[39m{\u001b[39;00mdate_time\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/rs39/lib/python3.9/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rs39/lib/python3.9/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/miniforge3/envs/rs39/lib/python3.9/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "# Get time in nice format\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time  = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "\n",
    "# SaveModels \n",
    "torch.save(reconstructor.state_dict(), f\"models/reconstructor_{date_time}.pt\")\n",
    "torch.save(sampling_agent.state_dict(), f\"models/sampling_agent_{date_time}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of final state is 590\n"
     ]
    }
   ],
   "source": [
    "from samprecon.samplers.spatial_transformers import differentiable_uniform_sampler\n",
    "def hard_decimation_of_state(high_freq_signal: torch.Tensor, rate: int, sampling_budget:int, num_classes:int):\n",
    "    blank_slate = torch.zeros_like(high_freq_signal)\n",
    "    seq_len = len(blank_slate)\n",
    "    samples = high_freq_signal[::rate][:sampling_budget]\n",
    "    for i,sample in enumerate(samples):\n",
    "        blank_slate[i*rate] = sample\n",
    "    # turn blank_slate into one hot\n",
    "    one_hot = F.one_hot(blank_slate.to(torch.long), num_classes=num_classes).view(1,-1,num_classes)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "sampling_agent.eval()\n",
    "reconstructor.eval()\n",
    "\n",
    "chosen_actions = []\n",
    "\n",
    "# Visually confirm proper reconstruction. \n",
    "num_examples = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_examples,2, figsize=(15,10))\n",
    "# Start with some previous state. \n",
    "\n",
    "\n",
    "for ne in range(num_examples):\n",
    "\n",
    "    regrets = []\n",
    "    val_ests = []\n",
    "    sbar.reset()\n",
    "\n",
    "    actions_taken = []\n",
    "\n",
    "    current_decimation_factor = torch.randint(1, int(max_decimation), (1,)).to(device)\n",
    "    decimation_rates = []\n",
    "\n",
    "    # Initial State\n",
    "    states = [\n",
    "        torch.cat(\n",
    "            (\n",
    "                torch.Tensor([current_decimation_factor]).to(device).view(1,1),\n",
    "                env.reset(current_decimation_factor).view(1, -1).to(device),\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for step in range(length_of_episode):\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        cur_state = states[-1]\n",
    "        action_probs = sampling_agent.act(cur_state[:sampling_budget+2])# CHECK why smapling_budget + 2\n",
    "        #print(\"Action probs are\", action_probs)\n",
    "        action_probs = action_probs.to(device)\n",
    "        sampled_action = torch.argmax(action_probs,dim=-1)\n",
    "\n",
    "        actions_taken.append(sampled_action.item())\n",
    "        \n",
    "        # CHECK: If we should have computation graph *not* cutting here\n",
    "        current_decimation_factor += decimation_steps[sampled_action.item()]\n",
    "        current_decimation_factor = torch.LongTensor([int(max(1,min(current_decimation_factor, max_decimation)))])\n",
    "        decimation_rates.append(current_decimation_factor.item())\n",
    "        \n",
    "        # TODO: tensorize\n",
    "        dec_steps = (\n",
    "            torch.arange(\n",
    "                0,\n",
    "                current_decimation_factor.item() * sampling_budget,\n",
    "                current_decimation_factor.item(),\n",
    "            )\n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        one_hot_cur_state = F.one_hot(\n",
    "            cur_state[0,1:sampling_budget+1].to(torch.long), num_classes=num_states\n",
    "        ).to(device)\n",
    "\n",
    "        non_amb_state = (\n",
    "            torch.cat((one_hot_cur_state, dec_steps), dim=-1).to(torch.float).to(device)\n",
    "        ).view(1,sampling_budget,-1)\n",
    "        val_ests.append(valueEst(non_amb_state).to(device))\n",
    "\n",
    "        new_state, regret, done = env.step(current_decimation_factor)\n",
    "\n",
    "        new_state_w_rate = torch.cat(\n",
    "                (\n",
    "                    torch.Tensor([current_decimation_factor]).to(device).view(1,1),\n",
    "                    new_state\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        states.append(new_state_w_rate.to(device))\n",
    "        regrets.append(regret)\n",
    "\n",
    "        sbar.set_description(f\"At step {step}, Regret: {regret}\")\n",
    "        sbar.update(1)\n",
    "\n",
    "    final_action = torch.argmax(sampling_agent.act(states[-1][:sampling_budget+2]),dim=-1)\n",
    "    final_dec_factor = current_decimation_factor.item() + final_action\n",
    "    final_state = torch.LongTensor(env.state_generator.sample(final_dec_factor, sampling_budget)).to(device)\n",
    "    final_state_oh = F.one_hot(final_state.view(1, -1).to(torch.long), num_classes=state_generator.max_state + 1).float()\n",
    "\n",
    "    #final_dec_state = hard_decimation_of_state(final_state, final_dec_factor, sampling_budget, num_states)\n",
    "    final_dec_state = differentiable_uniform_sampler(final_state_oh, final_dec_factor)\n",
    "\n",
    "    final_sm = F.softmax(env.reconstructor(final_dec_state,final_dec_factor),dim=-1)\n",
    "    final_reconstruction = torch.argmax(final_sm,dim=-1).cpu().detach().numpy().squeeze()\n",
    "\n",
    "\n",
    "    print(f\"Lenght of final state is {len(final_state)}\")\n",
    "\n",
    "    # Do plotting here\n",
    "    entire_x = np.arange(0,high_res_delta*(len(final_state)),step=high_res_delta)\n",
    "    axs[ne,0].plot(entire_x, final_state,drawstyle=\"steps-post\",label=\"Full resolution\")#, marker=\"^\",markersize=3)\n",
    "    # Plot Samples\n",
    "    dec_x = np.arange(0,(sampling_budget*final_dec_factor),step=finaL_dec_factor)\n",
    "    axs[ne,0].scatter(dec_x, final_state[::int(final_dec_factor)][:sampling_budget], label=\"Decimated\", marker=\"o\",color='r',s=30)\n",
    "    axs[ne,0].set_title(f\"Results for Experiment {ne+1}\")\n",
    "\n",
    "    # Plot Reconstrunction\n",
    "    axs[ne,0].plot(entire_x, final_reconstruction, label=\"Reconstruction\")#, marker=\"x\",markersize=3)\n",
    "    axs[ne,0].legend()\n",
    "\n",
    "    # History of Actions\n",
    "    axs[ne,1].plot(np.arange(len(decimation_rates)),decimation_rates)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "print(f\"Choices of actions were {chosen_actions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
