{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will change from the previous in that we will not have all possible action decimation rates available but lets say something along the lines of multiples. i.e. 1,2,4,8.\n",
    "\n",
    "We will also include the action in the state tape. So that it knows how fast it is going.\n",
    "\n",
    "Will probably also add replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rnd\n",
    "from time import time\n",
    "import torch.nn.functional as F\n",
    "from sp_sims.simulators.stochasticprocesses import BDStates\n",
    "from samprecon.environments.OneEpisodeEnvironments import MarkovianUniformCumulativeEnvironment\n",
    "from samprecon.samplers.agents import SoftmaxAgent \n",
    "from samprecon.reconstructors.NNReconstructors import RNNReconstructor\n",
    "from samprecon.utils.rl_utils import calculate_returns\n",
    "from samprecon.estimators.value_estimators import ValueFunc\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "plt.style.use('rose-pine-dawn')\n",
    "rnd.seed(int(time()))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimation factor is 6\n"
     ]
    }
   ],
   "source": [
    "# Generate Environments on which to learn\n",
    "high_res_delta = 1e-0  # For generating the dataset and later sample\n",
    "baseline_rates = {\"lam\": 1 / 10, \"mu\": 4 / 10}\n",
    "epochs = 300\n",
    "lenth_of_episode = 15\n",
    "step_path_length = 1\n",
    "sampling_budget = 32\n",
    "used_path_length = 64  # So that we can let the process reach stationarity and take samples from stationary distribution\n",
    "num_states = 4\n",
    "avg_span = np.mean(1 / np.array(list(baseline_rates.values())))\n",
    "max_decimation = (\n",
    "    avg_span / high_res_delta\n",
    ") * 4  # Max decimation factor #CHECK: Maybe not divide by 2\n",
    "decimation_steps = [-8,-4,-2,-1,0,1,2,4,8]\n",
    "current_decimation_factor = int(  # We can start somewhere in between\n",
    "    avg_span // high_res_delta\n",
    ")\n",
    "print(f\"Decimation factor is {current_decimation_factor}\")\n",
    "# Set random seed with time for randomnessj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ottersome/miniconda3/envs/research/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize context first\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_generator = BDStates(baseline_rates, high_res_delta, num_states)\n",
    "# sampling_arbiter.initialize_grad_hooks()\n",
    "reconstructor = RNNReconstructor(\n",
    "    amnt_states=num_states, max_decimation_rate=max_decimation\n",
    ").to(device)\n",
    "#reconstructor.initialize_grad_hooks()\n",
    "valueEst = ValueFunc(num_states).to(device)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a4c2e0bea14939bf4fa21d336a6f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81439f624d4f449a9248b81236057fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m non_amb_state \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     torch\u001b[39m.\u001b[39mcat((one_hot_cur_state, dec_steps), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m )\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m,sampling_budget,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m val_ests\u001b[39m.\u001b[39mappend(valueEst(non_amb_state)\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m new_state, regret, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(current_decimation_factor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m new_state_w_rate \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m         (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m             torch\u001b[39m.\u001b[39mTensor([current_decimation_factor])\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m         dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m states\u001b[39m.\u001b[39mappend(new_state_w_rate\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/samprecon/environments/OneEpisodeEnvironments.py:90\u001b[0m, in \u001b[0;36mMarkovianUniformCumulativeEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     81\u001b[0m new_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_generator\u001b[39m.\u001b[39msample(action, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_budget)\n\u001b[1;32m     83\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     85\u001b[0m new_state_oh \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(\n\u001b[1;32m     86\u001b[0m     new_state\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mlong),\n\u001b[1;32m     87\u001b[0m     num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_generator\u001b[39m.\u001b[39mmax_state \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     88\u001b[0m )\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 90\u001b[0m dec_state \u001b[39m=\u001b[39m differentiable_uniform_sampler(new_state_oh, action)\n\u001b[1;32m     92\u001b[0m reconstruction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreconstructor(\n\u001b[1;32m     93\u001b[0m     dec_state,\n\u001b[1;32m     94\u001b[0m     action,\n\u001b[1;32m     95\u001b[0m     \u001b[39m# 1 + torch.ceil(action.squeeze() * (self.sampling_budget - 1)),\u001b[39;00m\n\u001b[1;32m     96\u001b[0m )\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     98\u001b[0m logsoft_recon \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(reconstruction, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/samprecon/samplers/spatial_transformers.py:76\u001b[0m, in \u001b[0;36mdifferentiable_uniform_sampler\u001b[0;34m(input_signals, decimation_interval)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m# Esignal_length = len(input_signals)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# for now\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m mask \u001b[39m=\u001b[39m generate_sigmoid_mask(batch_size, signal_length, decimation_interval)\u001b[39m.\u001b[39mview(\n\u001b[1;32m     77\u001b[0m     \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     79\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mrepeat_interleave(input_signals\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m input_signals\u001b[39m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/samprecon/samplers/spatial_transformers.py:55\u001b[0m, in \u001b[0;36mgenerate_sigmoid_mask\u001b[0;34m(batch_size, signal_length, decimation_intervals, sharpness)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(signal_length):\n\u001b[1;32m     53\u001b[0m     jeep \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([j])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m     distance_to_nearest_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(\n\u001b[0;32m---> 55\u001b[0m         torch\u001b[39m.\u001b[39mfmod(jeep, decimation_intervals[i]),\n\u001b[1;32m     56\u001b[0m         \u001b[39m# decimation_intervals[i]\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         torch\u001b[39m.\u001b[39mTensor(decimation_intervals[i])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m         \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mfmod(jeep, decimation_intervals[i]),\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     distance \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor((distance_to_nearest_sample, \u001b[39m10\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     61\u001b[0m     distance_m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(distance)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "env = MarkovianUniformCumulativeEnvironment(\n",
    "    state_generator=state_generator,\n",
    "    reconstructor=reconstructor,\n",
    "    starting_decrate=current_decimation_factor,\n",
    "    sampling_budget=sampling_budget,\n",
    ")\n",
    "ebar = tqdm(range(epochs), desc=\"Epochs\", position=0)\n",
    "sampling_agent = SoftmaxAgent(sampling_budget+1, len(decimation_steps)).to(device)# +1 for the decimation factor\n",
    "# sampling_agent.initialize_grad_hooks()\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(reconstructor.parameters())\n",
    "    + list(sampling_agent.parameters())\n",
    "    + list(valueEst.parameters()),\n",
    "    lr=1e-2,\n",
    ")\n",
    "# val_opt = torch.optim.Adam(valueEst.parameters(), lr=1e-2)\n",
    "# Scheduler with warmpu\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "e_returns = []\n",
    "reconstructor_last_weights = list(reconstructor.state_dict().values())\n",
    "sampler_last_weights = list(sampling_agent.state_dict().values())\n",
    "for epoch in range(epochs):\n",
    "    # We generate a single step from the generator process\n",
    "    leave = epoch == epochs - 1\n",
    "\n",
    "    sbar = tqdm(range(lenth_of_episode), desc=\"Steps\", leave=leave, position=1)\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    val_ests = []\n",
    "    states = [\n",
    "        torch.cat(\n",
    "            (\n",
    "                torch.Tensor([current_decimation_factor]).to(device).view(1,1),\n",
    "                env.reset(current_decimation_factor).view(1, -1).to(device),\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for step in range(lenth_of_episode):\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        cur_state = states[-1]\n",
    "        action_probs = sampling_agent(cur_state[:sampling_budget+2]).to(device)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        sampled_action = (dist.sample()).to(\n",
    "            device\n",
    "        )  # So as to not sample 0 (and end up dividing by zero)\n",
    "        \n",
    "        current_decimation_factor += decimation_steps[sampled_action.item()]\n",
    "        current_decimation_factor = max(1,min(current_decimation_factor, max_decimation))\n",
    "        \n",
    "        dec_steps = (\n",
    "            torch.arange(\n",
    "                0,\n",
    "                current_decimation_factor * sampling_budget,\n",
    "                current_decimation_factor,\n",
    "            )\n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        one_hot_cur_state = F.one_hot(\n",
    "            cur_state[0,1:sampling_budget+1].to(torch.long), num_classes=num_states\n",
    "        ).to(device)\n",
    "        non_amb_state = (\n",
    "            torch.cat((one_hot_cur_state, dec_steps), dim=-1).to(torch.float).to(device)\n",
    "        ).view(1,sampling_budget,-1)\n",
    "        val_ests.append(valueEst(non_amb_state).to(device))\n",
    "\n",
    "        new_state, regret, done = env.step(current_decimation_factor)\n",
    "\n",
    "        new_state_w_rate = torch.cat(\n",
    "                (\n",
    "                    torch.Tensor([current_decimation_factor]).to(device).view(1,1),\n",
    "                    new_state\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        states.append(new_state_w_rate.to(device))\n",
    "        rewards.append(regret)\n",
    "        log_probs.append(dist.log_prob(current_decimation_factor - 1))\n",
    "\n",
    "        sbar.set_description(f\"At step {step}, Regret: {regret}\")\n",
    "        sbar.update(1)\n",
    "\n",
    "    returns = calculate_returns(rewards, gamma)\n",
    "    e_returns.append(returns[0].item())\n",
    "    policy_regrets = []\n",
    "    value_loss = []\n",
    "\n",
    "    for lp, val_est, r in zip(log_probs[:3], val_ests[:3], returns[:3]):\n",
    "        disadvantage = r - val_est.item()\n",
    "        policy_regrets.append(\n",
    "            -lp * disadvantage\n",
    "        )  # TODO: this might require a negative sign\n",
    "        value_loss.append(\n",
    "            F.mse_loss(val_est, torch.Tensor([r.item()]).view(1, -1).to(device))\n",
    "        )\n",
    "    # We update the whole thingko\n",
    "    policy_loss = torch.stack(policy_regrets).sum()\n",
    "    value_loss = torch.stack(value_loss).mean()\n",
    "\n",
    "    # optimze:\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # 🐛 Debugging\n",
    "    differences = []\n",
    "    for i, v in enumerate(sampling_agent.state_dict().values()):\n",
    "        differences.append(torch.sum(torch.abs(v - sampler_last_weights[i])))\n",
    "    differences_arbitrer = torch.sum(torch.tensor(differences))\n",
    "    # hard copy last weights\n",
    "    sampler_last_weights = [\n",
    "        copy.deepcopy(v) for v in sampling_agent.state_dict().values()\n",
    "    ]\n",
    "    differences = []\n",
    "    for i, v in enumerate(reconstructor.state_dict().values()):\n",
    "        differences.append(torch.sum(torch.abs(v - reconstructor_last_weights[i])))\n",
    "    differences_recon = torch.sum(torch.Tensor(differences))\n",
    "    reconstructor_last_weights = [\n",
    "        copy.deepcopy(v) for v in reconstructor.state_dict().values()\n",
    "    ]\n",
    "    print(\n",
    "        f\"Differences are : Sampler: {differences_arbitrer}, Reconstrctor: {differences_recon}\"\n",
    "    )\n",
    "\n",
    "    # 🐛 End Debuggin\n",
    "\n",
    "    moving_avg_loss = np.mean(e_returns[-3:]) if epoch > 3 else np.mean(e_returns)\n",
    "    ebar.set_description(f\"Epoch Mean Regret: {moving_avg_loss}\")\n",
    "    ebar.update(1)\n",
    "    # We get reward based on how close we got to maximum information\n",
    "# Show Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Regret\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (NLL)\")\n",
    "plt.plot(e_returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time in nice format\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time  = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "# SaveModels \n",
    "torch.save(reconstructor.state_dict(), f\"models/reconstructor_{date_time}.pt\")\n",
    "torch.save(sampling_agent.state_dict(), f\"models/sampling_agent_{date_time}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_decimation_of_state(high_freq_signal: torch.Tensor, rate: int, sampling_budget:int, num_classes:int):\n",
    "    blank_slate = torch.zeros_like(high_freq_signal)\n",
    "    seq_len = len(blank_slate)\n",
    "    samples = high_freq_signal[::rate.squeeze()][:sampling_budget]\n",
    "    for i,sample in enumerate(samples):\n",
    "        blank_slate[i*rate] = sample\n",
    "    # turn blank_slate into one hot\n",
    "    one_hot = F.one_hot(blank_slate.to(torch.long), num_classes=num_classes).view(1,-1,num_classes)\n",
    "    return one_hot\n",
    "\n",
    "sampling_agent.eval()\n",
    "reconstructor.eval()\n",
    "\n",
    "chosen_actions = []\n",
    "\n",
    "# Visually confirm proper reconstruction. \n",
    "num_examples = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_examples,1, figsize=(10,15))\n",
    "# Start with some previous state. \n",
    "\n",
    "states = [env.reset(current_decimation_factor).view(1, -1).to(device)]\n",
    "\n",
    "for ne in range(num_examples):\n",
    "    cur_state = states[-1]\n",
    "    # Maybe do argmax instead of sampling\n",
    "    action_probs = sampling_agent(cur_state[:sampling_budget])\n",
    "    #dist = torch.distributions.Categorical(action_probs)\n",
    "    #sampled_action = dist.sample() + 1 # So as to not sample 0 (and end up dividing by zero)\n",
    "    max_action = (torch.argmax(action_probs) + 1).view(1,1)\n",
    "    chosen_actions.append(max_action)\n",
    "\n",
    "    new_state = torch.Tensor(\n",
    "        state_generator.sample(max_action, sampling_budget)\n",
    "    ).to(device)\n",
    "\n",
    "    new_state_oh = F.one_hot(\n",
    "        new_state.view(1, -1).to(torch.long),\n",
    "        num_classes=state_generator.max_state + 1,\n",
    "    ).float()\n",
    "\n",
    "    dec_state = hard_decimation_of_state(new_state, max_action, sampling_budget, num_states)\n",
    "\n",
    "    reconstruction_probs = F.softmax(reconstructor(\n",
    "        dec_state.to(torch.float),\n",
    "        max_action.to(torch.float),\n",
    "    ),dim=-1)\n",
    "    reconstruction_states = torch.argmax(reconstruction_probs,dim=-1).cpu().detach().numpy().squeeze()\n",
    "\n",
    "    states.append(new_state)\n",
    "    new_state = new_state.cpu().detach().numpy()\n",
    "    max_action = max_action.cpu().detach().numpy()\n",
    "\n",
    "    # Do plotting here\n",
    "    axs[ne].plot(np.arange(len(new_state)), new_state, new_state,drawstyle=\"steps-post\",label=\"Full resolution\")#, marker=\"^\",markersize=3)\n",
    "    # Plot Samples\n",
    "    dec_x = np.arange(sampling_budget)*(int(max_action))\n",
    "    axs[ne].scatter(dec_x, new_state[::int(max_action)][:sampling_budget], label=\"Decimated\", marker=\"o\",color='r',s=30)\n",
    "    axs[ne].set_title(f\"Results for Experiment {ne}\")\n",
    "\n",
    "    # Plot Reconstrunction\n",
    "    axs[ne].plot(np.arange(len(reconstruction_states)), reconstruction_states, label=\"Reconstruction\")#, marker=\"x\",markersize=3)\n",
    "    axs[ne].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "print(f\"Choices of actions were {chosen_actions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
