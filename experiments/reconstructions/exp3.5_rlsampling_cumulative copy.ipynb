{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rnd\n",
    "from time import time\n",
    "import torch.nn.functional as F\n",
    "from sp_sims.simulators.stochasticprocesses import BDStates\n",
    "from samprecon.environments.OneEpisodeEnvironments import MarkovianUniformCumulativeEnvironment\n",
    "from samprecon.samplers.agents import SoftmaxAgent \n",
    "from samprecon.reconstructors.NNReconstructors import RNNReconstructor\n",
    "from samprecon.utils.rl_utils import calculate_returns\n",
    "from samprecon.estimators.value_estimators import ValueFunc\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "plt.style.use('rose-pine-dawn')\n",
    "rnd.seed(int(time()))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimation factor is 6\n"
     ]
    }
   ],
   "source": [
    "# Generate Environments on which to learn\n",
    "high_res_delta = 1e-0  # For generating the dataset and later sample\n",
    "baseline_rates = {\"lam\": 1 / 10, \"mu\": 4 / 10}\n",
    "epochs = 300\n",
    "lenth_of_episode = 15\n",
    "step_path_length = 1\n",
    "sampling_budget = 32\n",
    "used_path_length = 64  # So that we can let the process reach stationarity and take samples from stationary distribution\n",
    "num_states = 4\n",
    "avg_span = np.mean(1 / np.array(list(baseline_rates.values())))\n",
    "max_decimation = (\n",
    "    avg_span / high_res_delta\n",
    ") * 4  # Max decimation factor #CHECK: Maybe not divide by 2\n",
    "current_decimation_factor = int(  # We can start somewhere in between\n",
    "    avg_span // high_res_delta\n",
    ")\n",
    "print(f\"Decimation factor is {current_decimation_factor}\")\n",
    "# Set random seed with time for randomnessj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize context first\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_generator = BDStates(baseline_rates, high_res_delta, num_states)\n",
    "# sampling_arbiter.initialize_grad_hooks()\n",
    "reconstructor = RNNReconstructor(\n",
    "    amnt_states=num_states, max_decimation_rate=max_decimation\n",
    ").to(device)\n",
    "#reconstructor.initialize_grad_hooks()\n",
    "valueEst = ValueFunc(num_states)\n",
    "\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d146b98dc1a84d9fabc9fece6f46de30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccaee45ecee4686a2d9e869b132e1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ottersome/Research/SamplingReconstruction/exp3.5_rlsampling_cumulative.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.5_rlsampling_cumulative.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m one_hot_cur_state \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(cur_state[:sampling_budget]\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mlong), num_classes\u001b[39m=\u001b[39mnum_states)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.5_rlsampling_cumulative.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m non_amb_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((one_hot_cur_state, dec_steps), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.5_rlsampling_cumulative.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m val_ests\u001b[39m.\u001b[39mappend(valueEst(non_amb_state)\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.5_rlsampling_cumulative.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m new_state, regret, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(sampled_action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ottersome/Research/SamplingReconstruction/exp3.5_rlsampling_cumulative.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m states\u001b[39m.\u001b[39mappend(new_state\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/samprecon/estimators/value_estimators.py:23\u001b[0m, in \u001b[0;36mValueFunc.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     out,hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m     24\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(out[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:])\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "env = MarkovianUniformCumulativeEnvironment(\n",
    "    state_generator=state_generator,\n",
    "    reconstructor=reconstructor,\n",
    "    starting_decrate=current_decimation_factor,\n",
    "    sampling_budget=sampling_budget,\n",
    ")\n",
    "ebar = tqdm(range(epochs), desc=\"Epochs\", position=0)\n",
    "sampling_agent = SoftmaxAgent(sampling_budget, int(max_decimation)).to(device)\n",
    "# sampling_agent.initialize_grad_hooks()\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(reconstructor.parameters())\n",
    "    + list(sampling_agent.parameters())\n",
    "    + list(valueEst.parameters()),\n",
    "    lr=1e-2,\n",
    ")\n",
    "#val_opt = torch.optim.Adam(valueEst.parameters(), lr=1e-2)\n",
    "# Scheduler with warmpu\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "e_returns = []\n",
    "reconstructor_last_weights = list(reconstructor.state_dict().values())\n",
    "sampler_last_weights = list(sampling_agent.state_dict().values())\n",
    "for epoch in range(epochs):\n",
    "    # We generate a single step from the generator process\n",
    "    leave = epoch == epochs - 1\n",
    "\n",
    "    sbar = tqdm(range(lenth_of_episode), desc=\"Steps\", leave=leave, position=1)\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    val_ests = []\n",
    "    states = [env.reset(current_decimation_factor).view(1, -1).to(device)]\n",
    "\n",
    "    for step in range(lenth_of_episode):\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        cur_state = states[-1]\n",
    "        action_probs = sampling_agent(cur_state[:sampling_budget]).to(device)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        sampled_action = (dist.sample() + 1).to(\n",
    "            device\n",
    "        )  # So as to not sample 0 (and end up dividing by zero)\n",
    "        current_decimation_factor = sampled_action.item()\n",
    "        dec_steps = torch.arange(\n",
    "            0, current_decimation_factor * sampling_budget, current_decimation_factor\n",
    "        ).view(1,-1,1).to(device)\n",
    "        one_hot_cur_state = F.one_hot(cur_state[:sampling_budget].to(torch.long), num_classes=num_states).to(device)\n",
    "        non_amb_state = torch.cat((one_hot_cur_state, dec_steps), dim=-1).to(torch.float).to(device)\n",
    "        val_ests.append(valueEst(non_amb_state).to(device))\n",
    "\n",
    "        new_state, regret, done = env.step(sampled_action)\n",
    "\n",
    "        states.append(new_state.to(device))\n",
    "        rewards.append(regret)\n",
    "        log_probs.append(dist.log_prob(sampled_action - 1))\n",
    "\n",
    "        sbar.set_description(f\"At step {step}, Regret: {regret}\")\n",
    "        sbar.update(1)\n",
    "\n",
    "    returns = calculate_returns(rewards, gamma)\n",
    "    e_returns.append(returns[0].item())\n",
    "    policy_regrets = []\n",
    "    value_loss = []\n",
    "\n",
    "    for lp, val_est, r in zip(log_probs[:3], val_ests[:3], returns[:3]):\n",
    "        disadvantage = r - val_est.item()\n",
    "        policy_regrets.append(-lp * disadvantage)  # TODO: this might require a negative sign\n",
    "        value_loss.append(F.mse_loss(val_est, torch.Tensor([r.item()]).view(1,-1)))\n",
    "    # We update the whole thing\n",
    "    policy_loss = torch.stack(policy_regrets).sum()\n",
    "    value_loss = torch.stack(value_loss).mean()\n",
    "\n",
    "    # optimze:\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # ðŸ› Debugging\n",
    "    differences = []\n",
    "    for i, v in enumerate(sampling_agent.state_dict().values()):\n",
    "        differences.append(torch.sum(torch.abs(v - sampler_last_weights[i])))\n",
    "    differences_arbitrer = torch.sum(torch.tensor(differences))\n",
    "    # hard copy last weights\n",
    "    sampler_last_weights = [\n",
    "        copy.deepcopy(v) for v in sampling_agent.state_dict().values()\n",
    "    ]\n",
    "    differences = []\n",
    "    for i, v in enumerate(reconstructor.state_dict().values()):\n",
    "        differences.append(torch.sum(torch.abs(v - reconstructor_last_weights[i])))\n",
    "    differences_recon = torch.sum(torch.Tensor(differences))\n",
    "    reconstructor_last_weights = [\n",
    "        copy.deepcopy(v) for v in reconstructor.state_dict().values()\n",
    "    ]\n",
    "    print(\n",
    "        f\"Differences are : Sampler: {differences_arbitrer}, Reconstrctor: {differences_recon}\"\n",
    "    )\n",
    "\n",
    "    # ðŸ› End Debuggin\n",
    "\n",
    "    moving_avg_loss = np.mean(e_returns[-3:]) if epoch > 3 else np.mean(e_returns)\n",
    "    ebar.set_description(f\"Epoch Mean Regret: {moving_avg_loss}\")\n",
    "    ebar.update(1)\n",
    "    # We get reward based on how close we got to maximum information\n",
    "# Show Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Regret\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (NLL)\")\n",
    "plt.plot(e_returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time in nice format\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time  = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "# SaveModels \n",
    "torch.save(reconstructor.state_dict(), f\"models/reconstructor_{date_time}.pt\")\n",
    "torch.save(sampling_agent.state_dict(), f\"models/sampling_agent_{date_time}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_decimation_of_state(high_freq_signal: torch.Tensor, rate: int, sampling_budget:int, num_classes:int):\n",
    "    blank_slate = torch.zeros_like(high_freq_signal)\n",
    "    seq_len = len(blank_slate)\n",
    "    samples = high_freq_signal[::rate.squeeze()][:sampling_budget]\n",
    "    for i,sample in enumerate(samples):\n",
    "        blank_slate[i*rate] = sample\n",
    "    # turn blank_slate into one hot\n",
    "    one_hot = F.one_hot(blank_slate.to(torch.long), num_classes=num_classes).view(1,-1,num_classes)\n",
    "    return one_hot\n",
    "\n",
    "sampling_agent.eval()\n",
    "reconstructor.eval()\n",
    "\n",
    "chosen_actions = []\n",
    "\n",
    "# Visually confirm proper reconstruction. \n",
    "num_examples = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_examples,1, figsize=(10,15))\n",
    "# Start with some previous state. \n",
    "\n",
    "states = [env.reset(current_decimation_factor).view(1, -1).to(device)]\n",
    "\n",
    "for ne in range(num_examples):\n",
    "    cur_state = states[-1]\n",
    "    # Maybe do argmax instead of sampling\n",
    "    action_probs = sampling_agent(cur_state[:sampling_budget])\n",
    "    #dist = torch.distributions.Categorical(action_probs)\n",
    "    #sampled_action = dist.sample() + 1 # So as to not sample 0 (and end up dividing by zero)\n",
    "    max_action = (torch.argmax(action_probs) + 1).view(1,1)\n",
    "    chosen_actions.append(max_action)\n",
    "\n",
    "    new_state = torch.Tensor(\n",
    "        state_generator.sample(max_action, sampling_budget)\n",
    "    ).to(device)\n",
    "\n",
    "    new_state_oh = F.one_hot(\n",
    "        new_state.view(1, -1).to(torch.long),\n",
    "        num_classes=state_generator.max_state + 1,\n",
    "    ).float()\n",
    "\n",
    "    dec_state = hard_decimation_of_state(new_state, max_action, sampling_budget, num_states)\n",
    "\n",
    "    reconstruction_probs = F.softmax(reconstructor(\n",
    "        dec_state.to(torch.float),\n",
    "        max_action.to(torch.float),\n",
    "    ),dim=-1)\n",
    "    reconstruction_states = torch.argmax(reconstruction_probs,dim=-1).cpu().detach().numpy().squeeze()\n",
    "\n",
    "    states.append(new_state)\n",
    "    new_state = new_state.cpu().detach().numpy()\n",
    "    max_action = max_action.cpu().detach().numpy()\n",
    "\n",
    "    # Do plotting here\n",
    "    axs[ne].plot(np.arange(len(new_state)), new_state, new_state,drawstyle=\"steps-post\",label=\"Full resolution\")#, marker=\"^\",markersize=3)\n",
    "    # Plot Samples\n",
    "    dec_x = np.arange(sampling_budget)*(int(max_action))\n",
    "    axs[ne].scatter(dec_x, new_state[::int(max_action)][:sampling_budget], label=\"Decimated\", marker=\"o\",color='r',s=30)\n",
    "    axs[ne].set_title(f\"Results for Experiment {ne}\")\n",
    "\n",
    "    # Plot Reconstrunction\n",
    "    axs[ne].plot(np.arange(len(reconstruction_states)), reconstruction_states, label=\"Reconstruction\")#, marker=\"x\",markersize=3)\n",
    "    axs[ne].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "print(f\"Choices of actions were {chosen_actions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
