{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "- [x] When hitting restart(). Maybe do so from the state we left off at. Though it shoudlnt matter as much\n",
    "- [x] Train with random initial selection of sampling rates\n",
    "- [x] Change initial state. Could help with removing 0s\n",
    "- [x] Maybe add batching (if necessary)\n",
    "- [ ] Add current decimation rate extending the state from $\\text{numStates} + 1$ to $\\text{numStates} + 2$\n",
    "- [x] Change Adam to RMSProp\n",
    "- [x] Change the Policy Slower than the value funciton\n",
    "- [ ] Judge how close we are to $\\max_{i: i=j} P_\\Delta(i,j)$\n",
    "- [ ] Change step from spawning a new chain to just continuing the same one from before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Based on notebook exp3.6, in this one we will use a determinstic signal in the form of a Path MLE estimator. The estimator will initially assume perfect knowledge of Q and then estimate higher frequency paths with the viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path forimporting\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rnd\n",
    "from time import time\n",
    "import torch.nn.functional as F\n",
    "from sp_sims.simulators.stochasticprocesses import BDStates\n",
    "from sp_sims.utils.utils import get_q_mat\n",
    "from scipy.linalg import expm\n",
    "from samprecon.environments.Environments import MarkovianUniformCumulativeEnvironment\n",
    "from samprecon.samplers.agents import SoftmaxAgent \n",
    "from samprecon.reconstructors.NNReconstructors import MLEReconstructor\n",
    "from samprecon.utils.rl_utils import calculate_returns\n",
    "from samprecon.feedbacksigs.feedbacks import Reconstructor\n",
    "from samprecon.estimators.value_estimators import SequenceValue,ValueFunc\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Send random seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Initialize context first\n",
    "device = \"cpu\"\n",
    "# Check for Mac M1 and Cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"Running on GPU\")\n",
    "elif \"arm64\" in sys.version.lower():\n",
    "    device = \"mps\"\n",
    "    print(\"Running on Mac M1\")\n",
    "# for debugging üêõ\n",
    "#device = \"cpu\"\n",
    "\n",
    "plt.style.use('rose-pine-dawn')\n",
    "rnd.seed(int(time()))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimation factor is tensor([21])\n"
     ]
    }
   ],
   "source": [
    "# Generate Environments on which to learn\n",
    "high_res_delta = 1e-0  # For generating the dataset and later sample\n",
    "baseline_rates = {\"lam\": 1 / 10, \"mu\": 4 / 10}\n",
    "epochs = 400\n",
    "length_of_episode = 32\n",
    "sampling_budget = 1000\n",
    "epochs_b4_policy_update = 1\n",
    "value_learning_rate = 1e-2\n",
    "policy_learning_rate = 1e-2\n",
    "batch_size = 8\n",
    "#used_path_length = 64  # So that we can let the process reach stationarity and take samples from stationary distribution\n",
    "num_states = 12\n",
    "true_q = torch.from_numpy(get_q_mat(baseline_rates,num_states)).to(torch.float32).to(device)\n",
    "avg_span = np.mean(1 / np.array(list(baseline_rates.values())))\n",
    "max_decimation = (\n",
    "    avg_span / high_res_delta\n",
    ") * 4  # Max decimation factor #CHECK: Maybe not divide by 2\n",
    "decimation_steps = [-8,-4,-2,-1,0,1,2,4,8]\n",
    "# Easier\n",
    "#current_decimation_factor = torch.LongTensor(\n",
    "#    [int(avg_span // high_res_delta)]\n",
    "#).to(device)\n",
    "# Harder\n",
    "current_decimation_factor = torch.randint(1, int(max_decimation), (1,))\n",
    "\n",
    "print(f\"Decimation factor is {current_decimation_factor}\")\n",
    "# Set random seed with time for randomnessj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(nn.Module):\n",
    "    \"\"\" We make this complicated mostly to stick the the framework we started\"\"\"\n",
    "    def __init__(self, reduction: str):\n",
    "        super(Accuracy, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        pass\n",
    "    def forward(self, x, y):\n",
    "        vec = x!=y\n",
    "        final = torch.mean(vec) if self.reduction == \"mean\" else vec\n",
    "        return x!=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_generator = BDStates(baseline_rates, high_res_delta, num_states)\n",
    "padding_val = state_generator.padding_value\n",
    "# sampling_arbiter.initialize_grad_hooks()\n",
    "reconstructor = MLEReconstructor(\n",
    "    true_q, padding_val, sampling_budget,high_res_delta\n",
    ")\n",
    "recon_feedback = Reconstructor(num_states,reconstructor, Accuracy(reduction='none') )\n",
    "#reconstructor.initialize_grad_hooks()\n",
    "#valueEst = SequenceValue(num_states+1,1).to(device)\n",
    "valueEst = ValueFunc(sampling_budget+1, 1).to(device)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44e465b01bc4bc48304c4257ddc6ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing rates:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m cur_states[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m rate\n\u001b[1;32m     21\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m cur_state, regret, done,_ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#regrets[i] = regret.mean().item()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m regrets[i] \u001b[38;5;241m=\u001b[39m regret\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/environments/Environments.py:359\u001b[0m, in \u001b[0;36mMarkovianUniformCumulativeEnvironment.step\u001b[0;34m(self, cur_state, actions)\u001b[0m\n\u001b[1;32m    354\u001b[0m sampled_chain, fullres_chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_generator\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    355\u001b[0m     new_dec_period, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_budget, next_init_states\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    357\u001b[0m sampled_chain \u001b[38;5;241m=\u001b[39m sampled_chain\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 359\u001b[0m feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampled_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfullres_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_decimation_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_dec_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_budget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# actual_categories = torch.argmax(F.softmax(reconstruction, dim=-1), dim=-1)\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# self.logger.debug(f\"Reconstruction sum {actual_categories}\")\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# self.prev_state = new_state.to(torch.float)\u001b[39;00m\n\u001b[1;32m    370\u001b[0m new_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((new_dec_period, sampled_chain), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/feedbacksigs/feedbacks.py:20\u001b[0m, in \u001b[0;36mFeedbacks.__call__\u001b[0;34m(self, state, action, truth, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, truth, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feedback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/feedbacksigs/feedbacks.py:46\u001b[0m, in \u001b[0;36mReconstructor.get_feedback\u001b[0;34m(self, sampled_chain, action, truth, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m sampling_budget \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_budget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# new_oh = F.one_hot(\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#     state.view(1, -1).to(torch.long),\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     num_classes=self.num_states,\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# ).float()\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# dec_state = differentiable_uniform_sampler(new_oh, action)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstructor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampled_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_dec_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(truth\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#logsoft_recon = F.log_softmax(reconstruction, dim=-1).view(\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#    -1, reconstruction.shape[-1]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Check if criteroin is MSE or NLL\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSELoss\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/reconstructors/NNReconstructors.py:15\u001b[0m, in \u001b[0;36mReconstructor.__call__\u001b[0;34m(self, subsampled_signal, new_dec_period, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subsampled_signal, new_dec_period, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubsampled_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dec_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/reconstructors/NNReconstructors.py:141\u001b[0m, in \u001b[0;36mMLEReconstructor.reconstruct\u001b[0;34m(self, sampled_tape, dec_prop)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamp_budget \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    140\u001b[0m     viterbi_recon \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\n\u001b[0;32m--> 141\u001b[0m         \u001b[43msampling_viterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: deal with cur_dec = 1\u001b[39;49;00m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcur_dec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43msampled_tape\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43msampled_tape\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamp_budget \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    149\u001b[0m         reconstruction[b, s \u001b[38;5;241m*\u001b[39m cur_dec : (s \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m cur_dec ] \u001b[38;5;241m=\u001b[39m viterbi_recon[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../sp_sims/estimators/algos.py:130\u001b[0m, in \u001b[0;36msampling_viterbi\u001b[0;34m(num_hf_points, initial_state, last_state, trans_matrix)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, tape_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m tape_length:\n\u001b[1;32m    129\u001b[0m         repeated_mtp \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 130\u001b[0m             \u001b[43mmax_trans_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[1;32m    132\u001b[0m         evals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(repeated_mtp, trans_matrix)\n\u001b[1;32m    133\u001b[0m         vals, idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(evals,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create Evaluation Loop to See Delta-MSE Graph\n",
    "test_env = MarkovianUniformCumulativeEnvironment(\n",
    "    state_generator, recon_feedback,  400, sampling_budget\n",
    ")\n",
    "test_batch_size = 1\n",
    "rates = torch.logspace(1, 8, 100, base=2).to(device)\n",
    "regrets = torch.zeros_like(rates)\n",
    "tbar = tqdm(total=len(rates),desc='Testing rates')\n",
    "for i,rate in enumerate(rates):\n",
    "    \"\"\"\n",
    "    we only need cur_states to hold first and last column data. \n",
    "    First is the \"current decimation_rate\" and the last is the initial state for next simulation\n",
    "    \"\"\"\n",
    "    rate = int(rate)\n",
    "    cur_states = torch.zeros((test_batch_size, sampling_budget + 1), dtype=torch.long).to(device)\n",
    "    random_states = torch.randint(\n",
    "        0, num_states, (test_batch_size, 1)\n",
    "    ).to(device)\n",
    "    cur_states[:,-1] = random_states.squeeze()\n",
    "    cur_states[:, 0] = rate\n",
    "    action = torch.LongTensor([0]).to(device)\n",
    "\n",
    "    cur_state, regret, done,_ = test_env.step(cur_states.to(torch.long), action)\n",
    "\n",
    "    #regrets[i] = regret.mean().item()\n",
    "    regrets[i] = regret\n",
    "    tbar.update(1)\n",
    "\n",
    "# Plot regrets\n",
    "fig, axs = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "axs.plot(rates.cpu(), regrets.cpu(), label=\"Regret\")\n",
    "axs.set_xlabel(\"Decimation Proportion\")\n",
    "axs.set_ylabel(\"Regret (InAccuracy))\")\n",
    "axs.set_title(\"Regret vs Decimation Rate\")\n",
    "axs.set_xscale(\"log\", base=2)\n",
    "axs.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fe60953ea349ac9b4baf827e07d0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0486e19044407e974b00afdbce7ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (24976) must match the size of tensor b (199808) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#current_decimation_factor = torch.clamp(current_decimation_factor + selected_actions, 1, int(max_decimation))\u001b[39;00m\n\u001b[1;32m     66\u001b[0m val_ests\u001b[38;5;241m.\u001b[39mappend(valueEst(cur_state)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 68\u001b[0m new_state, regret, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(new_state\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     71\u001b[0m regrets\u001b[38;5;241m.\u001b[39mappend(regret)\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/environments/Environments.py:359\u001b[0m, in \u001b[0;36mMarkovianUniformCumulativeEnvironment.step\u001b[0;34m(self, cur_state, actions)\u001b[0m\n\u001b[1;32m    354\u001b[0m sampled_chain, fullres_chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_generator\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    355\u001b[0m     new_dec_period, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_budget, next_init_states\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    357\u001b[0m sampled_chain \u001b[38;5;241m=\u001b[39m sampled_chain\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 359\u001b[0m feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampled_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfullres_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_decimation_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_dec_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_budget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# actual_categories = torch.argmax(F.softmax(reconstruction, dim=-1), dim=-1)\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# self.logger.debug(f\"Reconstruction sum {actual_categories}\")\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# self.prev_state = new_state.to(torch.float)\u001b[39;00m\n\u001b[1;32m    370\u001b[0m new_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((new_dec_period, sampled_chain), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/feedbacksigs/feedbacks.py:20\u001b[0m, in \u001b[0;36mFeedbacks.__call__\u001b[0;34m(self, state, action, truth, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, truth, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feedback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/feedbacksigs/feedbacks.py:66\u001b[0m, in \u001b[0;36mReconstructor.get_feedback\u001b[0;34m(self, sampled_chain, action, truth, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     regret \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(reconstruction\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32), truth\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;241m.\u001b[39mview(truth\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# Accuracy Criterion \u001b[39;00m\n\u001b[1;32m     65\u001b[0m     regret \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;241m.\u001b[39mview(truth\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#regret = (\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#    #self.criterion(logsoft_recon, truth.to(torch.long).view(-1))\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#    self.criterion(reconstruction.to(torch.float32), truth.to(torch.float32))\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#    .view(reconstruction.shape[0], -1)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#    .mean(dim=-1)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: regret}\n",
      "File \u001b[0;32m~/miniforge3/envs/rs39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mAccuracy.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m----> 8\u001b[0m     vec \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43my\u001b[49m\n\u001b[1;32m      9\u001b[0m     final \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(vec) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m vec\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m!=\u001b[39my\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (24976) must match the size of tensor b (199808) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "env = MarkovianUniformCumulativeEnvironment(\n",
    "    state_generator=state_generator,\n",
    "    feedback=recon_feedback,\n",
    "    max_decimation = max_decimation,\n",
    "    sampling_budget=sampling_budget,\n",
    ")\n",
    "ebar = tqdm(range(epochs), desc=\"Epochs\", position=0)\n",
    "sampling_agent = SoftmaxAgent(sampling_budget+1, len(decimation_steps)).to(device)# +1 for the decimation factor\n",
    "# sampling_agent.initialize_grad_hooks()\n",
    "optimizer_policy = torch.optim.Adam(sampling_agent.parameters(), lr=policy_learning_rate)\n",
    "optimizer_value_est = torch.optim.Adam(valueEst.parameters(), lr=value_learning_rate)\n",
    "t_decsteps = torch.LongTensor(decimation_steps).to(device)\n",
    "# val_opt = torch.optim.Adam(valueEst.parameters(), lr=1e-2)\n",
    "\n",
    "# Scheduler with warmpu\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "e_returns = []\n",
    "est_loss = []\n",
    "val_losses = []\n",
    "value_last_weights = list(valueEst.state_dict().values())\n",
    "sampler_last_weights = list(sampling_agent.state_dict().values())\n",
    "sbar = tqdm(range(length_of_episode), desc=\"Steps\", leave=True, position=1)\n",
    "for epoch in range(epochs):\n",
    "    # We generate a single step from the generator process\n",
    "    regrets = []\n",
    "    log_probs = []\n",
    "    val_ests = []\n",
    "    sbar.reset()\n",
    "\n",
    "    current_decimation_factor = torch.randint(1, int(max_decimation), (batch_size,1)).to(device)\n",
    "\n",
    "    init_states = torch.randint(0, num_states, (batch_size,1)).to(device)\n",
    "\n",
    "    # Initial State\n",
    "    sampled_tape, fullres_tape = env.reset(current_decimation_factor, init_states)\n",
    "    states = [\n",
    "        torch.cat(\n",
    "            (\n",
    "                current_decimation_factor,\n",
    "                sampled_tape\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    ]\n",
    "    # TODO: randomly select current_decimation_factor within some bounds\n",
    "    for step in range(length_of_episode):\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        cur_state = states[-1].to(torch.float)\n",
    "        action_probs = sampling_agent.act(cur_state)\n",
    "        #print(\"Action probs are\", action_probs)\n",
    "        action_probs = action_probs.to(device)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        sampled_action = (dist.sample()).to(\n",
    "            device\n",
    "        )  # So as to not sample 0 (and end up dividing by zero)\n",
    "        \n",
    "        # pick from decimation_steps with indices specified in sampled_action\n",
    "        #current_decimation_factor += \n",
    "        selected_actions = t_decsteps[sampled_action].view(-1,1).to(device)\n",
    "        #current_decimation_factor = torch.clamp(current_decimation_factor + selected_actions, 1, int(max_decimation))\n",
    "        \n",
    "        val_ests.append(valueEst(cur_state).to(device))\n",
    "\n",
    "        new_state, regret, done, _ = env.step(cur_state.to(torch.long), selected_actions)\n",
    "\n",
    "        states.append(new_state.to(device))\n",
    "        regrets.append(regret)\n",
    "        log_probs.append(dist.log_prob(sampled_action))\n",
    "\n",
    "        #sbar.set_description(f\"At step {step}, Regret: {regret}\")\n",
    "        #sbar.update(1)\n",
    "\n",
    "    regrets = torch.stack(regrets).T\n",
    "\n",
    "    log_probs = torch.stack(log_probs).T \n",
    "    val_ests = torch.cat(val_ests, dim=-1)\n",
    "    returns = calculate_returns(regrets, gamma)\n",
    "\n",
    "    e_returns.append(returns[:,0].mean().item()) # For logging mostly\n",
    "    policy_regrets = []\n",
    "    value_loss = []\n",
    "\n",
    "    # TODO: add in batches\n",
    "    k = 0\n",
    "    for lp, val_est, reg, ret in zip(log_probs[:,:3].T, val_ests[:,:3].T, regrets[:,:3].T, returns[:,:3].T):\n",
    "        disadvantage = ret - val_est.detach() # Need detach otherwise we get double backprop error\n",
    "        policy_regrets.append(\n",
    "            -lp * disadvantage\n",
    "        )  # TODO: this might require a negative sign\n",
    "        #value_loss.append(\n",
    "        #    F.mse_loss(val_est, torch.Tensor([r.item()]).view(1, -1).to(device))\n",
    "        #)\n",
    "\n",
    "        # Change the value estimation to TD learning\n",
    "        current_value_est = val_est\n",
    "        next_value = val_ests[:,k+1]\n",
    "        target = reg + gamma * next_value\n",
    "        value_loss.append(\n",
    "            F.mse_loss(current_value_est, target.detach())\n",
    "        )\n",
    "\n",
    "        k+=1\n",
    "    # We update the whole thingko\n",
    "    policy_loss = torch.stack(policy_regrets).sum()\n",
    "    value_loss = torch.stack(value_loss).mean()\n",
    "\n",
    "    val_losses.append(value_loss.item())\n",
    "\n",
    "    # optimze:\n",
    "    optimizer_policy.zero_grad()\n",
    "    optimizer_value_est.zero_grad()\n",
    "    if epoch % epochs_b4_policy_update == 0:\n",
    "        policy_loss.backward()\n",
    "        optimizer_policy.step()\n",
    "    value_loss.backward()\n",
    "    optimizer_value_est.step()\n",
    "    #scheduler.step()\n",
    "\n",
    "    # üêõ Debugging\n",
    "    #differences = []\n",
    "    #for i, v in enumerate(sampling_agent.state_dict().values()):\n",
    "    #    differences.append(torch.sum(torch.abs(v - sampler_last_weights[i])))\n",
    "    #differences_arbitrer = torch.sum(torch.tensor(differences))\n",
    "    ## hard copy last weights\n",
    "    #sampler_last_weights = [\n",
    "    #    copy.deepcopy(v) for v in sampling_agent.state_dict().values()\n",
    "    #]\n",
    "    #differenes = []\n",
    "    #for i,v in enumerate(valueEst.state_dict().values()):\n",
    "    #    differences.append(torch.sum(torch.abs(v - value_last_weights[i])))\n",
    "    #differences_value = torch.sum(torch.Tensor(differences))\n",
    "    #value_last_weights = [\n",
    "    #    copy.deepcopy(v) for v in valueEst.state_dict().values()\n",
    "    #]\n",
    "    #print(\n",
    "    #    f\"Differences are : Sampler: {differences_arbitrer}, Reconstrctor: {differences_recon}, Value Estimator: {differences_value}\"\n",
    "    #)\n",
    "\n",
    "    # üêõ End Debuggin\n",
    "\n",
    "    moving_avg_loss = np.mean(e_returns[-3:]) if epoch > 3 else np.mean(e_returns)\n",
    "    ebar.set_description(f\"Epoch Mean Regret: {moving_avg_loss}\")\n",
    "    ebar.update(1)\n",
    "    # We get reward based on how close we got to maximum information\n",
    "# Show Losses\n",
    "fig,axs = plt.subplots(1,2, figsize=(20,5))\n",
    "axs[0].set_title(\"Average (actual) Reconstruction Regret\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss (NLL)\")\n",
    "axs[0].plot(e_returns)\n",
    "\n",
    "axs[1].set_title(\"Value Estimation Loss\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Loss (MSE)\")\n",
    "axs[1].plot(val_losses)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m date_time  \u001b[38;5;241m=\u001b[39m now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# SaveModels \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/reconstructor_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_time\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(sampling_agent\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/sampling_agent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.9/site-packages/torch/serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "# Get time in nice format\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time  = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "\n",
    "# SaveModels \n",
    "torch.save(reconstructor.state_dict(), f\"models/reconstructor_{date_time}.pt\")\n",
    "torch.save(sampling_agent.state_dict(), f\"models/sampling_agent_{date_time}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Example number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rac716/miniforge3/envs/rs39/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 73])) that is different to the input size (torch.Size([73])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/rac716/miniforge3/envs/rs39/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 76])) that is different to the input size (torch.Size([76])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 85\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#final_dec_state = hard_decimation_of_state(final_state, final_dec_factor, sampling_budget, num_states)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#final_dec_state = differentiable_uniform_sampler(final_state_oh, final_dec_factor)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mne\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m final_reconstruction \u001b[38;5;241m=\u001b[39m \u001b[43mreconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_dec_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m final_state \u001b[38;5;241m=\u001b[39m fullres_tape\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     88\u001b[0m final_dec_state \u001b[38;5;241m=\u001b[39m sampled_tape\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/reconstructors/NNReconstructors.py:15\u001b[0m, in \u001b[0;36mReconstructor.__call__\u001b[0;34m(self, subsampled_signal, new_dec_period, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subsampled_signal, new_dec_period, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubsampled_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dec_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/SamplingReconstruction/experiments/reconstructions/../../samprecon/reconstructors/NNReconstructors.py:133\u001b[0m, in \u001b[0;36mMLEReconstructor.reconstruct\u001b[0;34m(self, sampled_tape, dec_prop)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# OPTIM: Lots to optimizing to do here\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):  \u001b[38;5;66;03m# Batch\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     cur_dec \u001b[38;5;241m=\u001b[39m \u001b[43mdec_prop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    134\u001b[0m     P \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(expm(cur_dec \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhighres_delta \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ))\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_dec \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from samprecon.samplers.spatial_transformers import differentiable_uniform_sampler\n",
    "from samprecon.utils.utils import dec_rep_to_batched_rep, setup_logger\n",
    "\n",
    "def hard_decimation_of_state(high_freq_signal: torch.Tensor, rate: int, sampling_budget:int, num_classes:int):\n",
    "    blank_slate = torch.zeros_like(high_freq_signal)\n",
    "    seq_len = len(blank_slate)\n",
    "    samples = high_freq_signal[::rate][:sampling_budget]\n",
    "    for i,sample in enumerate(samples):\n",
    "        blank_slate[i*rate] = sample\n",
    "    # turn blank_slate into one hot\n",
    "    one_hot = F.one_hot(blank_slate.to(torch.long), num_classes=num_classes).view(1,-1,num_classes)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "sampling_agent.eval()\n",
    "\n",
    "chosen_actions = []\n",
    "\n",
    "# Visually confirm proper reconstruction. \n",
    "num_examples = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_examples,2, figsize=(30,20))\n",
    "# Start with some previous state. \n",
    "\n",
    "\n",
    "for ne in range(num_examples):\n",
    "    print(ne)\n",
    "\n",
    "    regrets = []\n",
    "    val_ests = []\n",
    "    sbar.reset()\n",
    "\n",
    "    actions_taken = []\n",
    "\n",
    "    current_decimation_factor = torch.randint(1, int(max_decimation), (1,)).to(device)\n",
    "    decimation_rates = []\n",
    "\n",
    "    init_states = torch.randint(0, num_states, (1,)).to(device)\n",
    "    sampled_tape, fullres_tape = env.reset(current_decimation_factor, init_states)\n",
    "    # Initial State\n",
    "    states = [\n",
    "        torch.cat(\n",
    "            (\n",
    "                current_decimation_factor.view(-1,1),\n",
    "                sampled_tape,\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for step in range(length_of_episode):\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        cur_state = states[-1]\n",
    "        action_probs = sampling_agent.act(cur_state.to(torch.float))# CHECK why smapling_budget + 2\n",
    "        #print(\"Action probs are\", action_probs)\n",
    "        action_probs = action_probs.to(device)\n",
    "        sampled_action = torch.argmax(action_probs,dim=-1)\n",
    "\n",
    "        actions_taken.append(sampled_action.item())\n",
    "        \n",
    "        # CHECK: If we should have computation graph *not* cutting here\n",
    "        current_decimation_factor += decimation_steps[sampled_action.item()]\n",
    "        current_decimation_factor = torch.LongTensor([int(max(1,min(current_decimation_factor, max_decimation)))]).to(device)\n",
    "        decimation_rates.append(current_decimation_factor.item())\n",
    "        \n",
    "        # TODO: tensorize\n",
    "        new_state, regret, done,_ = env.step(cur_state,sampled_action)\n",
    "\n",
    "\n",
    "        states.append(new_state)\n",
    "        regrets.append(regret)\n",
    "\n",
    "        sbar.set_description(f\"At step {step}, Regret: {regret}\")\n",
    "        sbar.update(1)\n",
    "\n",
    "    final_action = torch.argmax(sampling_agent.act(states[-1].to(torch.float)),dim=-1)\n",
    "    final_dec_factor = current_decimation_factor.item() + final_action\n",
    "    sampled_tape, fullres_tape = env.state_generator.sample(final_dec_factor.view(1,1), sampling_budget, states[-1][:,-1].view(1,1))\n",
    "    final_state_oh = F.one_hot(sampled_tape.view(1, -1).to(torch.long), num_classes=state_generator.max_state).float()\n",
    "\n",
    "    #final_dec_state = hard_decimation_of_state(final_state, final_dec_factor, sampling_budget, num_states)\n",
    "    #final_dec_state = differentiable_uniform_sampler(final_state_oh, final_dec_factor)\n",
    "\n",
    "    print(f\"Example number {ne}\")\n",
    "    final_reconstruction = reconstructor(sampled_tape, final_dec_factor)\n",
    "\n",
    "    final_state = fullres_tape.cpu().detach().numpy().squeeze()\n",
    "    final_dec_state = sampled_tape.cpu().detach().numpy().squeeze()\n",
    "    final_dec_factor = final_dec_factor.cpu().detach().numpy().squeeze()\n",
    "\n",
    "    print(f\"Lenght of final state is {len(final_state)}\")\n",
    "\n",
    "    # Do plotting here\n",
    "    entire_x = np.arange(0,high_res_delta*(len(final_state)),step=high_res_delta)\n",
    "    axs[ne,0].plot(entire_x, final_state,drawstyle=\"steps-post\",label=\"Full resolution\")#, marker=\"^\",markersize=3)\n",
    "    # Plot Samples\n",
    "    dec_x = np.arange(0,(sampling_budget*final_dec_factor),step=final_dec_factor)\n",
    "    axs[ne,0].scatter(dec_x, final_dec_state, label=\"Decimated\", marker=\"o\",color='r',s=30)\n",
    "    axs[ne,0].set_title(f\"Results for Experiment {ne+1}\")\n",
    "\n",
    "    # Plot Reconstrunction\n",
    "    #axs[ne,0].plot(entire_x, final_reconstruction, label=\"Reconstruction\")#, marker=\"x\",markersize=3)\n",
    "    axs[ne,0].plot(entire_x, final_reconstruction, label=\"Reconstruction\", marker=\"x\")#, marker=\"x\",markersize=3)\n",
    "    axs[ne,0].legend()\n",
    "\n",
    "    # History of Actions\n",
    "    axs[ne,1].plot(np.arange(len(decimation_rates)),decimation_rates)\n",
    "    axs[ne,1].set_title(\"History of Actions (Decimation Rates)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "print(f\"Choices of actions were {chosen_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
