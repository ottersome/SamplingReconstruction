{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will change from the previous in that we will not have all possible action decimation rates available but lets say something along the lines of multiples. i.e. 1,2,4,8.\n",
    "\n",
    "We will also include the action in the state tape. So that it knows how fast it is going.\n",
    "\n",
    "Will probably also add replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rnd\n",
    "from time import time\n",
    "import torch.nn.functional as F\n",
    "from sp_sims.simulators.stochasticprocesses import BDStates\n",
    "from samprecon.environments.OneEpisodeEnvironments import MarkovianUniformCumulativeEnvironment\n",
    "from samprecon.samplers.agents import SoftmaxAgent \n",
    "from samprecon.reconstructors.NNReconstructors import RNNReconstructor\n",
    "from samprecon.utils.rl_utils import calculate_returns\n",
    "from samprecon.estimators.value_estimators import ValueFunc\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "plt.style.use('rose-pine-dawn')\n",
    "rnd.seed(int(time()))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimation factor is tensor([[6]])\n"
     ]
    }
   ],
   "source": [
    "# Generate Environments on which to learn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "high_res_delta = 1e-0  # For generating the dataset and later sample\n",
    "baseline_rates = {\"lam\": 1 / 10, \"mu\": 4 / 10}\n",
    "epochs = 300\n",
    "lenth_of_episode = 15\n",
    "step_path_length = 1\n",
    "sampling_budget = 32\n",
    "used_path_length = 64  # So that we can let the process reach stationarity and take samples from stationary distribution\n",
    "num_states = 4\n",
    "avg_span = np.mean(1 / np.array(list(baseline_rates.values())))\n",
    "max_decimation = (\n",
    "    avg_span / high_res_delta\n",
    ") * 4  # Max decimation factor #CHECK: Maybe not divide by 2\n",
    "decimation_steps = [-8,-4,-2,-1,0,1,2,4,8]\n",
    "current_decimation_factor = torch.Tensor([int(  # We can start somewhere in between\n",
    "    avg_span // high_res_delta\n",
    ")]).to(torch.long).to(device).view(-1,1)# View is because we might want to batch it\n",
    "print(f\"Decimation factor is {current_decimation_factor}\")\n",
    "# Set random seed with time for randomnessj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize context first\n",
    "state_generator = BDStates(baseline_rates, high_res_delta, num_states)\n",
    "# sampling_arbiter.initialize_grad_hooks()\n",
    "reconstructor = RNNReconstructor(\n",
    "    amnt_states=num_states, max_decimation_rate=max_decimation\n",
    ").to(device)\n",
    "#reconstructor.initialize_grad_hooks()\n",
    "valueEst = ValueFunc(num_states).to(device)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44067971aa54498b010aebca706e74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6c6746dec240bea1dab19c51e3faf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m log_probs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m val_ests \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m states \u001b[39m=\u001b[39m [\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mcat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m             current_decimation_factor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m             env\u001b[39m.\u001b[39;49mreset(current_decimation_factor)\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lenth_of_episode):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# with torch.autograd.set_detect_anomaly(True):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rac716/Research/SamplingReconstruction/exp3.6.left_right.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     cur_state \u001b[39m=\u001b[39m states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got int"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "env = MarkovianUniformCumulativeEnvironment(\n",
    "    state_generator=state_generator,\n",
    "    reconstructor=reconstructor,\n",
    "    starting_decrate=current_decimation_factor,\n",
    "    sampling_budget=sampling_budget,\n",
    ")\n",
    "ebar = tqdm(range(epochs), desc=\"Epochs\", position=0)\n",
    "sampling_agent = SoftmaxAgent(sampling_budget+1, len(decimation_steps)).to(device)# +1 for the decimation factor\n",
    "# sampling_agent.initialize_grad_hooks()\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(reconstructor.parameters())\n",
    "    + list(sampling_agent.parameters())\n",
    "    + list(valueEst.parameters()),\n",
    "    lr=1e-2,\n",
    ")\n",
    "# val_opt = torch.optim.Adam(valueEst.parameters(), lr=1e-2)\n",
    "# Scheduler with warmpu\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "e_returns = []\n",
    "reconstructor_last_weights = list(reconstructor.state_dict().values())\n",
    "sampler_last_weights = list(sampling_agent.state_dict().values())\n",
    "for epoch in range(epochs):\n",
    "    # We generate a single step from the generator process\n",
    "    leave = epoch == epochs - 1\n",
    "\n",
    "    sbar = tqdm(range(lenth_of_episode), desc=\"Steps\", leave=leave, position=1)\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    val_ests = []\n",
    "    states = [\n",
    "        torch.cat(\n",
    "            (\n",
    "                current_decimation_factor,\n",
    "                env.reset(current_decimation_factor).view(1, -1).to(device),\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for step in range(lenth_of_episode):\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        cur_state = states[-1]\n",
    "        action_probs = sampling_agent(cur_state[:sampling_budget+2]).to(device)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        sampled_action = (dist.sample()).to(\n",
    "            device\n",
    "        )  # So as to not sample 0 (and end up dividing by zero)\n",
    "        \n",
    "        current_decimation_factor += torch.Tensor(decimation_steps[sampled_action.item()]).to(torch.long).to(device)\n",
    "        current_decimation_factor = max(1,min(current_decimation_factor, max_decimation))\n",
    "        \n",
    "        #TODO: Make batch friendly\n",
    "        dec_steps = (\n",
    "            torch.arange(\n",
    "                0,\n",
    "                current_decimation_factor.squeeze() * sampling_budget,\n",
    "                current_decimation_factor.squeeze(),\n",
    "            )\n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        one_hot_cur_state = F.one_hot(\n",
    "            cur_state[0,1:sampling_budget+1].to(torch.long), num_classes=num_states\n",
    "        ).to(device)\n",
    "        non_amb_state = (\n",
    "            torch.cat((one_hot_cur_state, dec_steps), dim=-1).to(torch.float).to(device)\n",
    "        ).view(1,sampling_budget,-1)\n",
    "        val_ests.append(valueEst(non_amb_state).to(device))\n",
    "\n",
    "        new_state, regret, done = env.step(current_decimation_factor)\n",
    "\n",
    "        new_state_w_rate = torch.cat(\n",
    "                (\n",
    "                    current_decimation_factor,\n",
    "                    new_state\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        states.append(new_state_w_rate.to(device))\n",
    "        rewards.append(regret)\n",
    "        log_probs.append(dist.log_prob(sampled_action))\n",
    "\n",
    "        sbar.set_description(f\"At step {step}, Regret: {regret}\")\n",
    "        sbar.update(1)\n",
    "\n",
    "    returns = calculate_returns(rewards, gamma)\n",
    "    e_returns.append(returns[0].item())\n",
    "    policy_regrets = []\n",
    "    value_loss = []\n",
    "\n",
    "    for lp, val_est, r in zip(log_probs[:3], val_ests[:3], returns[:3]):\n",
    "        disadvantage = r - val_est.item()\n",
    "        policy_regrets.append(\n",
    "            -lp * disadvantage\n",
    "        )  # TODO: this might require a negative sign\n",
    "        value_loss.append(\n",
    "            F.mse_loss(val_est, torch.Tensor([r.item()]).view(1, -1).to(device))\n",
    "        )\n",
    "    # We update the whole thingko\n",
    "    policy_loss = torch.stack(policy_regrets).sum()\n",
    "    value_loss = torch.stack(value_loss).mean()\n",
    "\n",
    "    # optimze:\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # ðŸ› Debugging\n",
    "    differences = []\n",
    "    for i, v in enumerate(sampling_agent.state_dict().values()):\n",
    "        differences.append(torch.sum(torch.abs(v - sampler_last_weights[i])))\n",
    "    differences_arbitrer = torch.sum(torch.tensor(differences))\n",
    "    # hard copy last weights\n",
    "    sampler_last_weights = [\n",
    "        copy.deepcopy(v) for v in sampling_agent.state_dict().values()\n",
    "    ]\n",
    "    differences = []\n",
    "    for i, v in enumerate(reconstructor.state_dict().values()):\n",
    "        differences.append(torch.sum(torch.abs(v - reconstructor_last_weights[i])))\n",
    "    differences_recon = torch.sum(torch.Tensor(differences))\n",
    "    reconstructor_last_weights = [\n",
    "        copy.deepcopy(v) for v in reconstructor.state_dict().values()\n",
    "    ]\n",
    "    print(\n",
    "        f\"Differences are : Sampler: {differences_arbitrer}, Reconstrctor: {differences_recon}\"\n",
    "    )\n",
    "\n",
    "    # ðŸ› End Debuggin\n",
    "\n",
    "    moving_avg_loss = np.mean(e_returns[-3:]) if epoch > 3 else np.mean(e_returns)\n",
    "    ebar.set_description(f\"Epoch Mean Regret: {moving_avg_loss}\")\n",
    "    ebar.update(1)\n",
    "    # We get reward based on how close we got to maximum information\n",
    "# Show Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Regret\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (NLL)\")\n",
    "plt.plot(e_returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time in nice format\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time  = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "# SaveModels \n",
    "torch.save(reconstructor.state_dict(), f\"models/reconstructor_{date_time}.pt\")\n",
    "torch.save(sampling_agent.state_dict(), f\"models/sampling_agent_{date_time}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_decimation_of_state(high_freq_signal: torch.Tensor, rate: int, sampling_budget:int, num_classes:int):\n",
    "    blank_slate = torch.zeros_like(high_freq_signal)\n",
    "    seq_len = len(blank_slate)\n",
    "    samples = high_freq_signal[::rate.squeeze()][:sampling_budget]\n",
    "    for i,sample in enumerate(samples):\n",
    "        blank_slate[i*rate] = sample\n",
    "    # turn blank_slate into one hot\n",
    "    one_hot = F.one_hot(blank_slate.to(torch.long), num_classes=num_classes).view(1,-1,num_classes)\n",
    "    return one_hot\n",
    "\n",
    "sampling_agent.eval()\n",
    "reconstructor.eval()\n",
    "\n",
    "chosen_actions = []\n",
    "\n",
    "# Visually confirm proper reconstruction. \n",
    "num_examples = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_examples,1, figsize=(10,15))\n",
    "# Start with some previous state. \n",
    "\n",
    "states = [env.reset(current_decimation_factor).view(1, -1).to(device)]\n",
    "\n",
    "for ne in range(num_examples):\n",
    "    cur_state = states[-1]\n",
    "    # Maybe do argmax instead of sampling\n",
    "    action_probs = sampling_agent(cur_state[:sampling_budget])\n",
    "    #dist = torch.distributions.Categorical(action_probs)\n",
    "    #sampled_action = dist.sample() + 1 # So as to not sample 0 (and end up dividing by zero)\n",
    "    max_action = (torch.argmax(action_probs) + 1).view(1,1)\n",
    "    chosen_actions.append(max_action)\n",
    "\n",
    "    new_state = torch.Tensor(\n",
    "        state_generator.sample(max_action, sampling_budget)\n",
    "    ).to(device)\n",
    "\n",
    "    new_state_oh = F.one_hot(\n",
    "        new_state.view(1, -1).to(torch.long),\n",
    "        num_classes=state_generator.max_state + 1,\n",
    "    ).float()\n",
    "\n",
    "    dec_state = hard_decimation_of_state(new_state, max_action, sampling_budget, num_states)\n",
    "\n",
    "    reconstruction_probs = F.softmax(reconstructor(\n",
    "        dec_state.to(torch.float),\n",
    "        max_action.to(torch.float),\n",
    "    ),dim=-1)\n",
    "    reconstruction_states = torch.argmax(reconstruction_probs,dim=-1).cpu().detach().numpy().squeeze()\n",
    "\n",
    "    states.append(new_state)\n",
    "    new_state = new_state.cpu().detach().numpy()\n",
    "    max_action = max_action.cpu().detach().numpy()\n",
    "\n",
    "    # Do plotting here\n",
    "    axs[ne].plot(np.arange(len(new_state)), new_state, new_state,drawstyle=\"steps-post\",label=\"Full resolution\")#, marker=\"^\",markersize=3)\n",
    "    # Plot Samples\n",
    "    dec_x = np.arange(sampling_budget)*(int(max_action))\n",
    "    axs[ne].scatter(dec_x, new_state[::int(max_action)][:sampling_budget], label=\"Decimated\", marker=\"o\",color='r',s=30)\n",
    "    axs[ne].set_title(f\"Results for Experiment {ne}\")\n",
    "\n",
    "    # Plot Reconstrunction\n",
    "    axs[ne].plot(np.arange(len(reconstruction_states)), reconstruction_states, label=\"Reconstruction\")#, marker=\"x\",markersize=3)\n",
    "    axs[ne].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "print(f\"Choices of actions were {chosen_actions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
